import gradio as gr
from openai import OpenAI
import os
import tempfile
from dotenv import load_dotenv
from markitdown import MarkItDown

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
api_base = os.getenv("OPENAI_API_BASE")
# åˆªé™¤å…¨åŸŸ clientï¼Œæ”¹ç”± generate_questions å‹•æ…‹åˆå§‹åŒ–

# âœ… åˆä½µå¤šæª”æ¡ˆæ–‡å­—

def extract_text_from_files(files):
    md = MarkItDown()
    merged_text = ""
    for f in files:
        result = md.convert(f.name)
        merged_text += result.text_content + "\n"
    return merged_text

# âœ… ç”¢å‡ºé¡Œç›®èˆ‡ç­”æ¡ˆï¼ˆæ ¹æ“šèªè¨€èˆ‡é¡Œå‹ï¼‰

def generate_questions(files, question_types, num_questions, lang, llm_key, baseurl, model=None):
    try:
        text = extract_text_from_files(files)
        trimmed_text = text[:200000]

        # å„ªå…ˆä½¿ç”¨ .envï¼Œå¦å‰‡ç”¨ UI å‚³å…¥å€¼
        key = os.getenv("OPENAI_API_KEY") or llm_key
        base = os.getenv("OPENAI_API_BASE") or baseurl
        model_name = model or "gpt-4.1"
        if not key or not base:
            return "âš ï¸ è«‹è¼¸å…¥ LLM key èˆ‡ baseurl", ""
        client = OpenAI(api_key=key, base_url=base)

        type_map = {
            "å–®é¸é¸æ“‡é¡Œ": {
                "zh-Hant": "å–®é¸é¸æ“‡é¡Œï¼ˆæ¯é¡Œå››å€‹é¸é …ï¼‰",
                "zh-Hans": "å•é€‰é€‰æ‹©é¢˜ï¼ˆæ¯é¢˜å››ä¸ªé€‰é¡¹ï¼‰",
                "en": "single choice question (4 options)",
                "ja": "å››æŠå•é¡Œ"
            },
            "å¤šé¸é¸æ“‡é¡Œ": {
                "zh-Hant": "å¤šé¸é¸æ“‡é¡Œï¼ˆæ¯é¡Œå››åˆ°äº”å€‹é¸é …ï¼‰",
                "zh-Hans": "å¤šé€‰é€‰æ‹©é¢˜ï¼ˆæ¯é¢˜å››åˆ°äº”ä¸ªé€‰é¡¹ï¼‰",
                "en": "multiple choice question (4-5 options)",
                "ja": "è¤‡æ•°é¸æŠå•é¡Œ"
            },
            "å•ç­”é¡Œ": {
                "zh-Hant": "ç°¡ç­”é¡Œ",
                "zh-Hans": "ç®€ç­”é¢˜",
                "en": "short answer",
                "ja": "çŸ­ç­”å¼å•é¡Œ"
            },
            "ç”³è«–é¡Œ": {
                "zh-Hant": "ç”³è«–é¡Œ",
                "zh-Hans": "ç”³è®ºé¢˜",
                "en": "essay question",
                "ja": "è¨˜è¿°å¼å•é¡Œ"
            }
        }

        prompt_map = {
            "ç¹é«”ä¸­æ–‡": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„å‡ºé¡Œè€…ï¼Œè«‹æ ¹æ“šä»¥ä¸‹å…§å®¹ï¼Œè¨­è¨ˆ {n} é¡Œä»¥ä¸‹é¡å‹çš„é¡Œç›®ï¼š{types}ã€‚æ¯é¡Œå¾Œé¢è«‹æ¨™è¨»ã€ç­”æ¡ˆã€‘ã€‚å…§å®¹å¦‚ä¸‹ï¼š\n{text}",
            "ç°¡é«”ä¸­æ–‡": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å‡ºé¢˜è€…ï¼Œè¯·æ ¹æ®ä»¥ä¸‹å†…å®¹ï¼Œè®¾è®¡ {n} é¢˜ä»¥ä¸‹ç±»å‹çš„é¢˜ç›®ï¼š{types}ã€‚æ¯é¢˜åé¢è¯·æ ‡æ³¨ã€ç­”æ¡ˆã€‘ã€‚å†…å®¹å¦‚ä¸‹ï¼š\n{text}",
            "English": "You are a professional exam writer. Based on the following content, generate {n} questions of types: {types}. Please mark the answer after each question using [Answer:]. Content:\n{text}",
            "æ—¥æœ¬èª": "ã‚ãªãŸã¯ãƒ—ãƒ­ã®å‡ºé¡Œè€…ã§ã™ã€‚ä»¥ä¸‹ã®å†…å®¹ã«åŸºã¥ã„ã¦ã€{types}ã‚’å«ã‚€{n}å•ã®å•é¡Œã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚å„å•é¡Œã®å¾Œã«ã€ç­”ãˆã€‘ã‚’ä»˜ã‘ã¦ãã ã•ã„ã€‚å†…å®¹ï¼š\n{text}"
        }

        lang_key_map = {
            "ç¹é«”ä¸­æ–‡": "zh-Hant",
            "ç°¡é«”ä¸­æ–‡": "zh-Hans",
            "English": "en",
            "æ—¥æœ¬èª": "ja"
        }

        lang_key = lang_key_map[lang]
        types_str = "ã€".join([type_map[t][lang_key] for t in question_types])
        prompt = prompt_map[lang].format(n=num_questions, types=types_str, text=trimmed_text)

        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": prompt}]
        )
        content = response.choices[0].message.content

        questions, answers = [], []
        for line in content.strip().split("\n"):
            if not line.strip():
                continue
            try:
                if "ã€ç­”æ¡ˆã€‘" in line:
                    q, a = line.split("ã€ç­”æ¡ˆã€‘", 1)
                elif "[Answer:" in line:
                    q, a = line.split("[Answer:", 1)
                    a = a.rstrip("]")
                elif "ã€ç­”ãˆã€‘" in line:
                    q, a = line.split("ã€ç­”ãˆã€‘", 1)
                else:
                    questions.append(line.strip())
                    answers.append("")
                    continue
                questions.append(q.strip())
                answers.append(a.strip())
            except Exception:
                questions.append(line.strip())
                answers.append("")

        if not questions:
            return "âš ï¸ ç„¡æ³•è§£æ AI å›å‚³å…§å®¹ï¼Œè«‹æª¢æŸ¥è¼¸å…¥å…§å®¹æˆ–ç¨å¾Œå†è©¦ã€‚", ""

        return "\n\n".join(questions), "\n\n".join(answers)
    except Exception as e:
        return f"âš ï¸ ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}", ""

# âœ… åŒ¯å‡º Markdown, Quizletï¼ˆTSVï¼‰

def export_files(questions_text, answers_text):
    md_path = tempfile.NamedTemporaryFile(delete=False, suffix=".md").name
    with open(md_path, "w", encoding="utf-8") as f:
        f.write("# ğŸ“˜ é¡Œç›® Questions\n\n" + questions_text + "\n\n# âœ… è§£ç­” Answers\n\n" + answers_text)

    quizlet_path = tempfile.NamedTemporaryFile(delete=False, suffix=".tsv").name
    with open(quizlet_path, "w", encoding="utf-8") as f:
        for q, a in zip(questions_text.split("\n\n"), answers_text.split("\n\n")):
            q_clean = q.replace("\n", " ").replace("\r", " ")
            a_clean = a.replace("\n", " ").replace("\r", " ")
            f.write(f"{q_clean}\t{a_clean}\n")

    return md_path, quizlet_path

# âœ… Gradio UI

# --- FastAPI + Gradio æ•´åˆ ---
from fastapi import FastAPI, UploadFile, File, Form
from fastapi.responses import JSONResponse
from typing import List, Optional
import uvicorn

def build_gradio_blocks():
    with gr.Blocks() as demo:
        gr.Markdown("# ğŸ“„ é€šç”¨ AI å‡ºé¡Œç³»çµ±ï¼ˆæ”¯æ´å¤šæª”ã€å¤šèªã€åŒ¯å‡ºæ ¼å¼ï¼‰")

        with gr.Row():
            with gr.Column():
                file_input = gr.File(
                    label="ä¸Šå‚³æ–‡ä»¶ï¼ˆå¯å¤šæª”ï¼‰",
                    file_types=[
                        ".pdf", ".ppt", ".pptx", ".doc", ".docx", ".xls", ".xlsx", ".csv",
                        ".jpg", ".jpeg", ".png", ".bmp", ".gif", ".tiff", ".webp",
                        ".mp3", ".wav", ".m4a", ".flac", ".ogg", ".aac", ".amr", ".wma", ".opus",
                        ".html", ".htm", ".json", ".xml", ".txt", ".md", ".rtf", ".log",
                        ".zip", ".epub"
                    ],
                    file_count="multiple"
                )
                lang = gr.Dropdown(["ç¹é«”ä¸­æ–‡", "ç°¡é«”ä¸­æ–‡", "English", "æ—¥æœ¬èª"], value="ç¹é«”ä¸­æ–‡", label="èªè¨€ Language")
                question_types = gr.CheckboxGroup(["å–®é¸é¸æ“‡é¡Œ", "å¤šé¸é¸æ“‡é¡Œ", "å•ç­”é¡Œ", "ç”³è«–é¡Œ"],
                                                  label="é¸æ“‡é¡Œå‹ï¼ˆå¯è¤‡é¸ï¼‰",
                                                  value=["å–®é¸é¸æ“‡é¡Œ"])
                num_questions = gr.Slider(1, 20, value=10, step=1, label="é¡Œç›®æ•¸é‡")
                llm_key = gr.Textbox(label="LLM Key (ä¸æœƒå„²å­˜)", type="password", placeholder="è«‹è¼¸å…¥ä½ çš„ OpenAI API Key")
                baseurl = gr.Textbox(label="Base URL (å¦‚ https://api.openai.com/v1)",value="https://api.openai.com/v1", placeholder="è«‹è¼¸å…¥ API Base URL")
                model_box = gr.Textbox(label="Model åç¨±", value="gpt-4.1", placeholder="å¦‚ gpt-4.1, gpt-3.5-turbo, ...")
                generate_btn = gr.Button("âœï¸ é–‹å§‹å‡ºé¡Œ")

            with gr.Column():
                qbox = gr.Textbox(label="ğŸ“˜ é¡Œç›® Questions", lines=15)
                abox = gr.Textbox(label="âœ… è§£ç­” Answers", lines=15)
                export_btn = gr.Button("ğŸ“¤ åŒ¯å‡º Markdown / Quizlet")
                md_out = gr.File(label="ğŸ“ Markdown æª”ä¸‹è¼‰")
                quizlet_out = gr.File(label="ğŸ“‹ Quizlet (TSV) æª”ä¸‹è¼‰")


        generate_btn.click(fn=generate_questions,
                           inputs=[file_input, question_types, num_questions, lang, llm_key, baseurl, model_box],
                           outputs=[qbox, abox])

        export_btn.click(fn=export_files,
                         inputs=[qbox, abox],
                         outputs=[md_out, quizlet_out])
    return demo

if __name__ == "__main__":
    demo = build_gradio_blocks()
    demo.launch()

# --- FastAPI API ä»‹é¢ ---
from fastapi import FastAPI, UploadFile, File, Form
from fastapi.responses import JSONResponse
from typing import List, Optional
import uvicorn

api_app = FastAPI(title="AI å‡ºé¡Œç³»çµ± API")

@api_app.post("/api/generate")
async def api_generate(
    files: List[UploadFile] = File(...),
    question_types: List[str] = Form(...),
    num_questions: int = Form(...),
    lang: str = Form(...),
    llm_key: Optional[str] = Form(None),
    baseurl: Optional[str] = Form(None),
    model: Optional[str] = Form(None)
):
    # å°‡ UploadFile è½‰ç‚ºè‡¨æ™‚æª”æ¡ˆç‰©ä»¶ï¼Œèˆ‡ Gradio è¡Œç‚ºä¸€è‡´
    temp_files = []
    for f in files:
        temp = tempfile.NamedTemporaryFile(delete=False)
        temp.write(await f.read())
        temp.flush()
        temp_files.append(temp)
        temp.name = temp.name  # ä¿æŒä»‹é¢ä¸€è‡´

    # å‘¼å«åŸæœ¬çš„å‡ºé¡Œé‚è¼¯
    questions, answers = generate_questions(
        temp_files, question_types, num_questions, lang, llm_key, baseurl, model
    )

    # é—œé–‰è‡¨æ™‚æª”æ¡ˆ
    for temp in temp_files:
        temp.close()

    return JSONResponse({"questions": questions, "answers": answers})

# è‹¥è¦å•Ÿå‹• API ä¼ºæœå™¨ï¼Œè«‹åŸ·è¡Œï¼š
# uvicorn app:api_app --host 0.0.0.0 --port 7861
